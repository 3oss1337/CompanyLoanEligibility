# -*- coding: utf-8 -*-
"""Copy of Project_Template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1No3Q3qigjYwOHrLOldUib9Hys4kepN40

# This notebook is prepared by ApplAi's Technical And Training Depratment.
- Please Don't use it outside the training without asking for permission as it's considered as Violation of Intellectual property rights

![ApplAi's Logo](https://media-exp1.licdn.com/dms/image/C4E0BAQHGLGltI2rzuQ/company-logo_200_200/0?e=2159024400&v=beta&t=adq8rNV09dPC6egdJMnfARt6Aq0TC9bSomFvFtm50WM)

### Task 1: Importing libraries and Exploring the Dataset.

### Task 2: Definining Exploratory Data Analysis with an overview of the whole project .

### Task 3: Checking missing values and Outliers & Creating visual methods to analyze the data.

### Task 4: creat a model that fits the data

### Task 5: creating an accurecy table

###

### Task 1: Importing libraries and Exploring the Dataset.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

loan = pd.read_csv('Comp loan.csv')

"""### Task 2: Definining Exploratory Data Analysis with an overview of the whole project"""

loan.head(10)

loan.shape

loan.info()

loan.dtypes

loan.duplicated().sum()

loan.isna().sum()

loan['completion_status'].unique()

loan=loan.drop(columns=['RATE_ID_FOR_industry_type'],axis=1)
loan=loan.drop(columns=['RATE_ID_FOR_judgement_lien_time'],axis=1)
loan=loan.drop(columns=['INPUT_VALUE_ID_FOR_judgement_lien_time'],axis=1)
loan=loan.drop(columns=['RATE_ID_FOR_funded_last_30'],axis=1)
loan=loan.drop(columns=['RATE_ID_FOR_fsr'],axis=1)
loan=loan.drop(columns=['PERCENT_OWN_owner_3'],axis=1)
loan=loan.drop(columns=['CAP_AMOUNT_owner_3'],axis=1)
loan=loan.drop(columns=['RATE_owner_3'],axis=1)
loan=loan.drop(columns=['owner_3_score'],axis=1)
loan=loan.drop(columns=['deal_application_thread_id'],axis=1)
loan=loan.drop(columns=['id'],axis=1)
loan=loan.drop(columns=['Unnamed: 0'],axis=1)
loan=loan.drop(columns=['RATE_ID_FOR_avg_net_deposits'],axis=1)

loan.isnull().sum()

"""### Task 3: Checking missing values and Outliers & Creating visual methods to analyze the data."""

loan['owner_1_score'].fillna( loan['owner_1_score'].mean() , inplace=True)
loan['CAP_AMOUNT_owner_1'].fillna( loan['CAP_AMOUNT_owner_1'].mean() , inplace=True)
loan['PERCENT_OWN_owner_1'].fillna( loan['PERCENT_OWN_owner_1'].mean() , inplace=True)
loan['owner_2_score'].fillna( 0 , inplace=True)
loan['CAP_AMOUNT_owner_2'].fillna( 0 , inplace=True)
loan['PERCENT_OWN_owner_2'].fillna( 0 , inplace=True)
loan['years_in_business'].fillna( loan['years_in_business'].mean() , inplace=True)
loan['fsr'].fillna( loan['fsr'].mean() , inplace=True)
loan['INPUT_VALUE_ID_FOR_tax_lien_count'].fillna( loan['INPUT_VALUE_ID_FOR_tax_lien_count'].mean() , inplace=True)
loan['INPUT_VALUE_ID_FOR_current_position'].fillna( loan['INPUT_VALUE_ID_FOR_current_position'].mean() , inplace=True)
loan['INPUT_VALUE_ID_FOR_avg_net_deposits'].fillna( loan['INPUT_VALUE_ID_FOR_avg_net_deposits'].mean() , inplace=True)
loan['INPUT_VALUE_owner_4'].fillna( loan['INPUT_VALUE_owner_4'].mean() , inplace=True)
loan['CAP_AMOUNT_owner_4'].fillna( loan['CAP_AMOUNT_owner_4'].mean() , inplace=True)
loan['PERCENT_OWN_owner_4'].fillna( loan['PERCENT_OWN_owner_4'].mean() , inplace=True)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
#----------------------------------------------------------------------------
loan['RATE_owner_1'] = label_encoder.fit_transform(loan['RATE_owner_1'])
loan['RATE_owner_2'] = label_encoder.fit_transform(loan['RATE_owner_2'])
loan['location'] = label_encoder.fit_transform(loan['location'])
loan['RATE_ID_FOR_years_in_business'] = label_encoder.fit_transform(loan['RATE_ID_FOR_years_in_business'])
loan['RATE_ID_FOR_location'] = label_encoder.fit_transform(loan['RATE_ID_FOR_location'])
loan['RATE_ID_FOR_num_negative_days'] = label_encoder.fit_transform(loan['RATE_ID_FOR_num_negative_days'])
loan['RATE_ID_FOR_tax_lien_count'] = label_encoder.fit_transform(loan['RATE_ID_FOR_tax_lien_count'])
loan['RATE_ID_FOR_current_position'] = label_encoder.fit_transform(loan['RATE_ID_FOR_current_position'])
loan['INPUT_VALUE_ID_FOR_industry_type'] = label_encoder.fit_transform(loan['INPUT_VALUE_ID_FOR_industry_type'])
loan['RATE_owner_4'] = label_encoder.fit_transform(loan['RATE_owner_4'])
loan['completion_status']=loan['completion_status'].replace('Default',0)
loan['completion_status']=loan['completion_status'].replace('Paid off with renewal',1)
loan['completion_status']=loan['completion_status'].replace('Paid via discounted payoff',2)
loan['completion_status']=loan['completion_status'].replace('Paid in full',3)
loan['RATE_ID_FOR_judgement_lien_percent'] = label_encoder.fit_transform(loan['RATE_ID_FOR_judgement_lien_percent'])
loan['RATE_ID_FOR_judgement_lien_amount'] = label_encoder.fit_transform(loan['RATE_ID_FOR_judgement_lien_amount'])
loan['RATE_ID_FOR_num_deposits'] = label_encoder.fit_transform(loan['RATE_ID_FOR_num_deposits'])
loan['RATE_ID_FOR_monthly_gross'] = label_encoder.fit_transform(loan['RATE_ID_FOR_monthly_gross'])
loan['RATE_ID_FOR_average_ledger'] = label_encoder.fit_transform(loan['RATE_ID_FOR_average_ledger'])
loan['RATE_ID_FOR_fc_margin'] = label_encoder.fit_transform(loan['RATE_ID_FOR_fc_margin'])
loan['RATE_ID_FOR_tax_lien_amount'] = label_encoder.fit_transform(loan['RATE_ID_FOR_tax_lien_amount'])
loan['RATE_ID_FOR_tax_lien_percent'] = label_encoder.fit_transform(loan['RATE_ID_FOR_tax_lien_percent'])
loan['funded_last_30'] = label_encoder.fit_transform(loan['funded_last_30'])

loan.isna().sum()

n =[100,200,300,400,500,600,700,800,900]
plt.figure(figsize = (18,5))
sns.distplot(loan['owner_1_score'],bins =10,rug=True,hist_kws={'edgecolor': 'black'})
plt.xlim(0,100)
plt.title("First owner Credit score")
plt.xlabel("credit score")
plt.ylabel("Amount")
plt.xticks(n)
plt.show()

from sklearn.preprocessing import StandardScaler
from scipy.stats import shapiro
import warnings
warnings.filterwarnings('ignore')
#-----------------------------------------------------------
X = np.array(loan.drop(['completion_status'],axis=1))
y = np.array(loan['completion_status'])

X = X.reshape(X.shape[0],-1)
y= y.reshape(y.shape[0],-1)
#------------------------------------------------------------
X_scaled = StandardScaler().fit_transform(X)

"""### Task 4: creat a model that fits the data

# Logistic Regression Model

---
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
#---------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 4)
classifier=LogisticRegression()
classifier.fit (X_train ,y_train)
y_pred=classifier.predict(X_test)
#---------------------------------------------------------
cm = confusion_matrix(y_test, y_pred)
cr=classification_report(y_test, y_pred)
acc = accuracy_score(y_test,y_pred)
#---------------------------------------------------------
print("The Confusion Matrix:\n", cm)
print("\n\n-----------------------------------------------------------------------------\n\n")
print("The Classification Report:\n\n\n",cr)
print("\n\n-----------------------------------------------------------------------------\n\n")
print("The Accuracy Score:",acc)
print("\n\n-----------------------------------------------------------------------------\n\n")

"""#KNN Model

---


"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
#---------------------------------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=4)
model = KNeighborsClassifier(n_neighbors=1)
model.fit(X_train, y_train)
y_pred_test=model.predict(X_test)
y_pred_train=model.predict(X_train)
#---------------------------------------------------------------------------------------
acc_testKNN = accuracy_score(y_test, y_pred_test)
acc_train =accuracy_score(y_train, y_pred_train)
#---------------------------------------------------------------------------------------
print("Accuracy \"Test\":", acc_testKNN)
print("Accuracy \"Train\":", acc_train)

#split data into training and test data (80% versus 20%)
X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.2,random_state=4)
#further split traning data into training and validation data (90% versus 10%)
X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=4)
#----------------------------------------------------------------------------------------------------------
max_depth_range = range(1, 16)
val_results = []
train_results = []
#-----------------------------------------------------------------------------------------------------------
for k in max_depth_range:
    clf_2 = KNeighborsClassifier(n_neighbors=k)
    clf_2 = clf_2.fit(X_train_new, y_train_new)
    #accuracy on training
    pred_train_new = clf_2.predict(X_train_new)
    train_score = accuracy_score(y_train_new, pred_train_new)
    train_results.append(train_score)
    #accuracy on validation data
    pred_val = clf_2.predict(X_val)
    val_score = accuracy_score(y_val, pred_val)
    val_results.append(val_score)
#-------------------------------------------------------------------------------------------------------------
plt.plot(max_depth_range, val_results, 'g-', label='Val score')
plt.plot(max_depth_range, train_results, 'r-', label='Train score')
plt.ylabel('Score')
plt.xlabel('Model complexity: K Neighbors')
plt.legend()
plt.grid(True)
plt.show()

"""# Decision Tree Model

---


"""

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
#----------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)
DTree = DecisionTreeClassifier(criterion='entropy',max_depth=15)
DTree = DTree.fit(X_train,y_train)
y_pred_test= DTree.predict(X_test)
y_pred_train= DTree.predict(X_train)
#-----------------------------------------------------------
acc_testDTree = accuracy_score(y_test, y_pred_test)
acc_train =accuracy_score(y_train, y_pred_train)
#-----------------------------------------------------------
print("Accuracy \"Test\":", acc_testDTree)
print("Accuracy \"Train\":", acc_train)

#split data into training and test data (80% versus 20%)
X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.2,random_state=4)
#further split traning data into training and validation data (90% versus 10%)
X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=4)
#----------------------------------------------------------------------------------------------------------
max_depth_range = range(1, 20)
val_results = []
train_results = []
#-----------------------------------------------------------------------------------------------------------
for k in max_depth_range:
    clf_2 = DecisionTreeClassifier(max_depth=k)
    clf_2 = clf_2.fit(X_train_new, y_train_new)
    #accuracy on training
    pred_train_new = clf_2.predict(X_train_new)
    train_score = accuracy_score(y_train_new, pred_train_new)
    train_results.append(train_score)
    #accuracy on validation data
    pred_val = clf_2.predict(X_val)
    val_score = accuracy_score(y_val, pred_val)
    val_results.append(val_score)
#-------------------------------------------------------------------------------------------------------------
plt.plot(max_depth_range, val_results, 'g-', label='Val score')
plt.plot(max_depth_range, train_results, 'r-', label='Train score')
plt.ylabel('Score')
plt.xlabel('Model complexity: K Neighbors')
plt.legend()
plt.grid(True)
plt.show()

"""### Task 5: creating an accurecy table"""

Models = ['Logisitic Regression', 'KNN', 'Decision Tree']
accuracies = []
accuracies.append(acc)
accuracies.append(acc_testKNN)
accuracies.append(acc_testDTree)

accuracy_data = pd.DataFrame({'Model': Models, 'Accuracy': accuracies})
accuracy_data

"""### Conclusion

---
# The best model for this data is the decision tree since it has the best accuracy on the test data.

## JobLib

---
"""

import joblib

joblib_file = "Company_loan_model"
joblib.dump(DTree, joblib_file)



loaded_model = joblib.load(open(joblib_file, 'rb'))

pred_Y = loaded_model.predict(X_test)
result = np.round(accuracy_score(y_test, pred_Y) ,2)
print(result)